# ğŸª Big Mart Sales Analysis using PySpark

## ğŸ“Œ Project Overview
This project demonstrates an **end-to-end Big Data ETL pipeline** built using **PySpark**, **HDFS**, and **Hive** to analyze Big Mart sales data.  
The pipeline ingests raw CSV data, performs large-scale data cleaning and transformations, applies aggregations and window functions, optimizes Spark jobs, and stores the processed data in **Hive tables using Parquet format**.

This project simulates **real-world data engineering workflows** and processes **1M+ records**.

---

## ğŸ› ï¸ Technologies Used
- PySpark
- Apache Spark
- HDFS
- Apache Hive
- Spark SQL
- Parquet
- Linux
- VS Code
- Git & GitHub
- Databricks (Optional)

---

## ğŸ“‚ Project Structure
bigmart-pyspark-project/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ raw/
â”‚ â””â”€â”€ Train.csv
â”‚
â”œâ”€â”€ spark/
â”‚ â”œâ”€â”€ ingest_data.py
â”‚ â”œâ”€â”€ data_cleaning.py
â”‚ â”œâ”€â”€ transformations.py
â”‚ â””â”€â”€ aggregations.py
â”‚
â”œâ”€â”€ hive/
â”‚ â””â”€â”€ create_tables.sql
â”‚
â”œâ”€â”€ output/
â”‚ â””â”€â”€ parquet/
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt'


---

## ğŸ“Š Dataset
- **Dataset Name:** Big Mart Sales Dataset  
- **Source:** Kaggle  
- **Link:** https://www.kaggle.com/datasets/brijbhushannanda1979/bigmart-sales-data  

To simulate **large-scale processing**, the dataset was duplicated using PySpark to generate **1M+ records**.

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Start Hadoop & Hive Services
```bash
start-dfs.sh
hive --service metastore &
hive --service hiveserver2 &
